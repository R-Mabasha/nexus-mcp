# ==========================================
# Lightning-Crew Multi-LLM Configuration
# ==========================================
# We use LiteLLM for routing. This means you can plug in OpenAI, Anthropic, Gemini, OpenRouter, or Local models easily.

# 1. Choose Your Primary Swarm Engine
# Supported Formats:
# "openai/gpt-4o"
# "anthropic/claude-3-5-sonnet-20241022"
# "ollama/llama3.1" (Local)
# "openrouter/anthropic/claude-3.5-sonnet"
SWARM_MODEL="openai/gpt-4o"

# 2. Assign the Necessary API Keys (Only the ones you need)
OPENAI_API_KEY="sk-..."
ANTHROPIC_API_KEY="sk-ant-..."
OPENROUTER_API_KEY="..."
GEMINI_API_KEY="..."

# 3. Local Models or Custom Endpoints (Optional)
# If using LM Studio, Ollama, or vLLM, specify the local base URL here.
# Leave blank for standard cloud providers.
# CUSTOM_API_BASE="http://localhost:11434/v1"

# 4. Agent Configuration Settings
# Max loops the Crew QA will attempt before breaking sequence
MAX_SWARM_RETRIES=3

# 5. Developer Integrations (Optional)
# GITHUB_TOKEN="ghp_..."
# GITHUB_REPO="your-username/your-repo-name"
